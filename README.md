# Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models
The repo is for paper [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459).
## Reproducibility
For safety and ethics concerns, we do not publicly release the complete jailbreak code. Based on the description in our paper, we provide enough details for reproducibility. The data and code access will be granted via submitting a form indicating the researchersâ€™ affiliation and the intention of use.
## Citation
```
@article{yang2024jigsaw,
  title={Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models},
  author={Yang, Hao and Qu, Lizhen and Shareghi, Ehsan and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2410.11459},
  year={2024}
}
```
